Code Readability (variable names, structure, formatting) -> 5
Check if you read the code easily without extra effort. Sub-criteria: Use descriptive names for variables and functions (e.g., "calculateTotalPrice" instead of "calc"); apply consistent indentation (4 spaces) and line length (max 100 characters); break code into small functions (max 30 lines each); replace magic numbers with named constants; avoid clever one-liners. Metrics: Average function length in lines; percentage of meaningful names (detect via linter rules); code style violations count (from tools like ESLint or Black). Poor readability slows down team reviews.
Documentation (comments, docstrings, README) -> 4
Assess if documentation explains the code without reading the source. Sub-criteria: Add docstrings to every public function/class/module (include parameters, returns, exceptions, examples); use inline comments only for tricky parts; provide a complete README with setup, usage, architecture overview, and contribution guide; document public APIs with examples. Metrics: Percentage of functions with docstrings (>90% target); comment density (10-20 lines per 100 code lines); README completeness score (check for required sections). Missing documentation delays new developer onboarding.
Functional Correctness (code does what it should, without logical errors) -> 5
Verify the code meets requirements without bugs. Sub-criteria: Match outputs to specifications on normal inputs; handle all specified scenarios; avoid common logical flaws (off-by-one, incorrect bounds checks, wrong operator precedence); integrate correctly with dependencies. Metrics: Number of failing acceptance tests; static analysis bug count (from tools like SonarQube); manual review findings on logic errors. Incorrect functionality breaks the application.
Tests (unit coverage, integration tests, edge cases) -> 5
Evaluate test quality and coverage. Sub-criteria: Write unit tests for every function (mock external calls); include integration tests for module interactions; cover edge cases (empty inputs, nulls, maximum values, invalid data); assert both happy paths and error handling. Metrics: Code coverage percentage (>85% branch coverage target via tools like Jest or pytest-cov); tests per function ratio (>1); mutation testing score (kill rate >80%); edge case coverage checklist. Weak tests let regressions slip into production.
Complexity (cyclomatic complexity, avoiding excessive nesting) -> 4
Measure how hard the code is to understand and test. Sub-criteria: Keep cyclomatic complexity low (max 10 per function); limit nesting depth to 4 levels; extract complex logic into separate functions; avoid long chains of conditions. Metrics: Average cyclomatic complexity per function (via tools like Lizard or Radon); maximum nesting depth; cognitive complexity score (from SonarQube). High complexity raises bug risk.
Code Duplication (no repetitions, refactoring) -> 4
Detect repeated code blocks. Sub-criteria: Identify identical or similar blocks (>10 lines); extract into shared functions or classes; follow DRY principle; refactor duplicates during changes. Metrics: Duplicated lines percentage (<5% target via tools like CPD or Simian); number of code clones; duplication blocks count. Duplicates multiply maintenance effort.
Performance (optimization, avoiding bottlenecks) -> 4
Check execution efficiency. Sub-criteria: Use appropriate algorithms (e.g., O(n log n) sort instead of O(nÂ²)); avoid unnecessary loops or database queries; cache repeated computations; profile and fix slow paths. Metrics: Execution time for key operations (benchmark in ms); memory usage (peak MB); Big O analysis per function; profiler hotspot percentage. Bad performance hurts user experience at scale.
Security (protection against vulnerabilities, input validation) -> 5
Ensure protection from common attacks. Sub-criteria: Validate and sanitize all user inputs; prevent injection attacks (SQL, command, XSS); use secure defaults (HTTPS, secure cookies); handle secrets properly (no hardcoding); follow OWASP guidelines. Metrics: Vulnerability count from scanners (Snyk, Bandit, OWASP Dependency-Check); input validation coverage percentage; known CVE matches in dependencies. Security flaws expose data to breaches.
Maintainability (compliance with SOLID, modularity) -> 5
Assess ease of future changes. Sub-criteria: Follow SOLID principles (Single Responsibility, Open-Closed, etc.); keep low coupling and high cohesion; separate concerns into modules; avoid large classes (>300 lines). Metrics: Maintainability Index (>85 target via tools); afferent/efferent coupling; class/method size averages; refactor churn rate. Low maintainability increases long-term costs.
Compliance with Standards (coding style, team conventions) -> 4
Verify adherence to rules. Sub-criteria: Run linters/formatters automatically; follow language-specific guides (e.g., Google Style or Airbnb); use consistent naming conventions; enforce via pre-commit hooks. Metrics: Linter warning/error count (target 0); style compliance percentage; convention violation rate in diffs. Inconsistent style disrupts team collaboration.